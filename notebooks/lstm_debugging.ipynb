{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9570ae07f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "HIDDEN_DIM = 5\n",
    "SEQ_LEN = 7\n",
    "lstm = nn.LSTM(input_size=INPUT_DIM, \n",
    "               hidden_size=HIDDEN_DIM, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(SEQ_LEN, 1, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = [torch.randn(1, INPUT_DIM) for _ in range(SEQ_LEN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6490, -0.0774, -0.6042]],\n",
       "\n",
       "        [[-0.5731,  0.9578,  1.5043]],\n",
       "\n",
       "        [[-1.3950,  0.8008, -0.6619]],\n",
       "\n",
       "        [[ 1.2563,  0.5000,  0.0402]],\n",
       "\n",
       "        [[ 0.4647, -0.0312, -0.0939]],\n",
       "\n",
       "        [[-0.6191, -0.6363, -0.4242]],\n",
       "\n",
       "        [[-2.0272,  1.3015, -0.6293]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hidden_state, cell_state) = lstm(inputs, (hidden.view(1, SEQ_LEN, HIDDEN_DIM), cell.view(1, SEQ_LEN, HIDDEN_DIM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 1, 5]), (torch.Size([1, 7, 5]), torch.Size([1, 7, 5])))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), (hidden_state.size(), cell_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0014, -0.3442, -0.3233,  0.1272,  0.0628]],\n",
       "\n",
       "        [[ 0.1822, -0.1008, -0.1856,  0.2728, -0.1620]],\n",
       "\n",
       "        [[-0.3205, -0.0206,  0.1476,  0.2962, -0.0751]],\n",
       "\n",
       "        [[-0.3481, -0.1549,  0.2820, -0.0994,  0.3369]],\n",
       "\n",
       "        [[-0.0077,  0.0343, -0.2153,  0.0807,  0.1262]],\n",
       "\n",
       "        [[-0.1799,  0.0330,  0.1802,  0.1951,  0.3317]],\n",
       "\n",
       "        [[-0.0519,  0.0721,  0.2354, -0.2404,  0.1540]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 5]), torch.Size([7, 1, 5]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0123, -0.2199,  0.2875, -0.1356,  0.0766],\n",
       "         [ 0.3382,  0.2510,  0.6205,  0.4422,  0.0732],\n",
       "         [-0.0388,  0.0336,  0.0089, -0.1430, -0.0643],\n",
       "         [-0.0184,  0.0949, -0.0476, -0.0689, -0.1356],\n",
       "         [ 0.0110,  0.0574,  0.1300,  0.0568,  0.0288],\n",
       "         [ 0.2608,  0.1801,  0.5237,  0.1864,  0.0636],\n",
       "         [-0.0150,  0.0053,  0.2304, -0.1897, -0.0862]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, cell = (torch.randn(SEQ_LEN, 1, HIDDEN_DIM), torch.randn(SEQ_LEN, 1, HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6490, -0.0774, -0.6042]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0014, -0.3442, -0.3233,  0.1272,  0.0628]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[[ 0.0014, -0.3442, -0.3233,  0.1272,  0.0628]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[ 0.0031, -0.4773, -0.7585,  0.2320,  0.1027]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1, (hid1, cell1) =  lstm(inputs[0].unsqueeze(0), (hidden[0].unsqueeze(0), cell[0].unsqueeze(0)))\n",
    "out1, hid1, cell1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 3]), (torch.Size([1, 1, 5]), torch.Size([1, 1, 5])))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].unsqueeze(0).size(), (hidden[0].unsqueeze(0).size(), cell[0].unsqueeze(0).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 5]), (torch.Size([1, 1, 5]), torch.Size([1, 1, 5])))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.size(), (hid1.size(), cell1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying `nn.LSTM` module\n",
    "\n",
    "## What’s the difference between hidden and output in PyTorch LSTM?\n",
    "\n",
    "According to Pytorch documentation\n",
    "\n",
    "```py\n",
    "\"\"\"\n",
    "Outputs: output, (h_n, c_n)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "There are 2 types of usage of `nn.lstm` module.\n",
    "\n",
    "- **TYPE 1 (BULK MODE):** Feed all the input in a bulk to the `lstm` module\n",
    "- **TYPE 2 (LOOP MODE):** Feed each element of the input to the `lstm` module in a loop\n",
    "\n",
    "## How to interpret the BULK MODE?\n",
    "\n",
    "- `Outputs` comprises all the hidden states in the last layer (“last” `depth-wise`, not time-wise). \n",
    "- $(h_n,c_n)$ comprises the hidden states after the last time step, $t=n$, so you could potentially feed them into another LSTM.\n",
    "\n",
    "![image](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "Simple example to show that both the approach **may not** generate same output for identical problem definition.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Say I have a sentence: _i love my city kolkata very much_. And we want to feed the sentence to a `nn.lstm` module using above 2 approaches.\n",
    "\n",
    "We have a sequence length = 7 here.\n",
    "\n",
    "We need to convert each token `[\"i\", \"love\", \"my\", \"city\", \"kolkata\", \"very\", \"much\"]` to an embedding. For this demo we generate an random embedding of dimension of `3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 7\n",
    "IMPUT_DIM = EMBED_DIM = 3\n",
    "HIDDEN_DIM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually `input dimension` and `embedding dimension` are same. As word ambeddings are the input to the lstm module. We can use both the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -1.1524, -0.2506]],\n",
       "\n",
       "        [[-0.4339,  0.8487, -1.5551]],\n",
       "\n",
       "        [[-0.3414,  1.8530,  0.4681]],\n",
       "\n",
       "        [[-0.1577,  1.4437,  0.2660]],\n",
       "\n",
       "        [[ 0.1665,  1.5863,  0.9463]],\n",
       "\n",
       "        [[-0.8437,  0.9318,  1.2590]],\n",
       "\n",
       "        [[ 2.0050,  0.0537,  0.6181]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "inputs = torch.randn(SEQ_LEN, 1, INPUT_DIM)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "\"i\"       = [[-1.1258, -1.1524, -0.2506]],\n",
    "\"love\"    = [[-0.4339,  0.8487, -1.5551]],\n",
    "\"my\"      = [[-0.3414,  1.8530,  0.4681]],\n",
    "\"city\"    = [[-0.1577,  1.4437,  0.2660]],\n",
    "\"kolkata\" = [[ 0.1665,  1.5863,  0.9463]],\n",
    "\"very\"    = [[-0.8437,  0.9318,  1.2590]],\n",
    "\"much\"    = [[ 2.0050,  0.0537,  0.6181]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare our `lstm` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=INPUT_DIM, \n",
    "               hidden_size=HIDDEN_DIM, \n",
    "               batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting fact: `nn.LSTM()` returns a function and we assigned the function in a variable name `lstm`.\n",
    "\n",
    "The function `lstm()` expects all the argument `inputs, (hidden, cell)` as 3D tensor. \n",
    "\n",
    "Now we can pass the entire embedding/input matrix `inputs` to the `lstm()` function.  If you are using TYPE 1, then we can call `lstm()` in 2 ways: \n",
    "\n",
    "- Without `(hidden, cell)`. Then system initializes the `(hidden,cell)` with 0\n",
    "- With custom `(hidden, cell)` initialization\n",
    "\n",
    "Syntax:\n",
    "\n",
    "```py\n",
    "out, (hidden, cell) =  lstm(inputs)\n",
    "```\n",
    "\n",
    "Now in many LSTM example we will see this notation where bulk inputs are fed to the `lstm()` module. The confusion arrises when we see example where TYPE 2 approach is used and each input in fed over loop. However we can show both TYPE 1 and TYPE 2 approach are same if we use same `(hidden, cell)` initialization for both the cases. \n",
    "\n",
    "But technically there is a slight catch. And that is related to the tensor shape for `(hidden,cell)`. \n",
    "\n",
    "In practice, LSTM is a recurrent network. Which takes one embedding for one word and the corresponding `(hidden,cell)` and returns `out, (hidden, cell)`. Now in bulk approach, all are sent together.\n",
    "\n",
    "Let's initialize `(hidden, cell)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, cell = (torch.randn(SEQ_LEN, 1, HIDDEN_DIM), torch.randn(SEQ_LEN, 1, HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[-0.0422,  0.1172,  0.1255,  0.3234,  0.8920]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1, (hid_1, cell_1) =  lstm(inputs[0].unsqueeze(0), (hidden[0].unsqueeze(0), cell[0].unsqueeze(0)))\n",
    "out_1, hid_1, cell_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2, (hid_2, cell_2) =  lstm(inputs[1].unsqueeze(0), (hid_1, cell_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0721,  0.0187, -0.0858, -0.0551, -0.0058]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.0721,  0.0187, -0.0858, -0.0551, -0.0058]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.2038,  0.0295, -0.2052, -0.1197, -0.0175]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_2, (hid_2, cell_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TYPE 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_type_1, (hidden_type_1, cell_type_1) = lstm(inputs, (hidden.view(1, SEQ_LEN, HIDDEN_DIM), cell.view(1, SEQ_LEN, HIDDEN_DIM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]],\n",
       "\n",
       "        [[-0.0631, -0.3371, -0.1404, -0.3782,  0.0775]],\n",
       "\n",
       "        [[ 0.1218,  0.5558,  0.0761, -0.3177, -0.3491]],\n",
       "\n",
       "        [[ 0.0722,  0.0822,  0.2336, -0.0847, -0.2263]],\n",
       "\n",
       "        [[ 0.0533,  0.0153,  0.1362, -0.3684, -0.1344]],\n",
       "\n",
       "        [[ 0.0011, -0.0427, -0.1056,  0.1165,  0.2600]],\n",
       "\n",
       "        [[-0.0602, -0.1055,  0.0908,  0.1777, -0.0081]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_type_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see, the first row of `out_type_1` is similar to `out_1`. But the subsequent rows of `out_type_1` are differnet, as the returned `(hidden,cell)` are fed back into `lstm()`. \n",
    "\n",
    "This arises one question. We initially initialized the `(hidden, cell)` for all the `7` tokens, but it seems redundant. But that's not the case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unroll the bulk and try to regenerate the `output_type_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.0422,  0.1172,  0.1255,  0.3234,  0.8920]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1, (hid_1, cell_1) =  lstm(inputs[0].unsqueeze(0), (hidden[0].unsqueeze(0), cell[0].unsqueeze(0)))\n",
    "out_1, (hid_1, cell_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is fine. But the twist comes next. See I am not passing the `(hid_1, cell_1)` for the token `inputs[1]` rather i am passing a reshaped version of ` (hidden[1], cell[1])`  and that is creating the exact replica of `output_type_1[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0631, -0.3371, -0.1404, -0.3782,  0.0775]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.0631, -0.3371, -0.1404, -0.3782,  0.0775]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.1009, -0.5193, -0.2378, -1.1299,  0.2654]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_2, (hid_2, cell_2) =  lstm(inputs[1].unsqueeze(0), (hidden[1].unsqueeze(0), cell[1].unsqueeze(0)))\n",
    "out_2, (hid_2, cell_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go on like this....\n",
    "\n",
    "**Observation:**\n",
    "\n",
    "If you see carefully, it seems, in bulk mode (in the above unrolled version), each output is not generated by the previous `(hidden, cell)` i.e $(h_{t-1}, c_{t-1})$ as seen by the above example (but the results are matching for `bulk output` and `unrolled version` of bulk output). \n",
    "\n",
    "\n",
    "## Following LSTM defition:\n",
    "\n",
    "Now lets feed the $(h_{t-1}, c_{t-1})$ (as per the original LSTM definition) to generate the next `out`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.0229,  0.0760,  0.0806,  0.0651,  0.4780]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.0422,  0.1172,  0.1255,  0.3234,  0.8920]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1, (hid_1, cell_1) =  lstm(inputs[0].unsqueeze(0), (hidden[0].unsqueeze(0), cell[0].unsqueeze(0)))\n",
    "out_1, (hid_1, cell_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0721,  0.0187, -0.0858, -0.0551, -0.0058]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.0721,  0.0187, -0.0858, -0.0551, -0.0058]]],\n",
       "         grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.2038,  0.0295, -0.2052, -0.1197, -0.0175]]],\n",
       "         grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_2, (hid_2, cell_2) =  lstm(inputs[1].unsqueeze(0), (hid_1, cell_1))\n",
    "out_2, (hid_2, cell_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "The `out_2` is different from `output_type_1[1]` (both denoting the second element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9570ae07f0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1815,  0.0289,  0.5399]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.1815,  0.0289,  0.5399]]], grad_fn=<StackBackward>), tensor([[[-0.3697,  0.0903,  1.3455]]], grad_fn=<StackBackward>))\n",
      "tensor([[[0.0569, 0.1538, 0.0446]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.0569, 0.1538, 0.0446]]], grad_fn=<StackBackward>), tensor([[[0.1005, 0.3668, 1.2979]]], grad_fn=<StackBackward>))\n",
      "tensor([[[ 0.0991, -0.0257, -0.0674]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[ 0.0991, -0.0257, -0.0674]]], grad_fn=<StackBackward>), tensor([[[ 0.1282, -0.0523, -0.1263]]], grad_fn=<StackBackward>))\n",
      "tensor([[[ 0.1365, -0.2060,  0.0026]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[ 0.1365, -0.2060,  0.0026]]], grad_fn=<StackBackward>), tensor([[[ 0.2138, -0.3261,  0.0117]]], grad_fn=<StackBackward>))\n",
      "tensor([[[0.2096, 0.0675, 0.0626]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.2096, 0.0675, 0.0626]]], grad_fn=<StackBackward>), tensor([[[0.3354, 0.2321, 0.4211]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    print(out)\n",
    "    print(hidden)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3341,  0.1119,  0.2212]],\n",
      "\n",
      "        [[-0.0564,  0.2298,  0.0374]],\n",
      "\n",
      "        [[-0.0412,  0.0749, -0.0919]],\n",
      "\n",
      "        [[ 0.0481, -0.1281, -0.0022]],\n",
      "\n",
      "        [[ 0.1721,  0.0864,  0.0560]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[0.1721, 0.0864, 0.0560]]], grad_fn=<StackBackward>), tensor([[[0.2710, 0.2993, 0.4017]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
